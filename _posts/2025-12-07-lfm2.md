# LFM2 

Modern LMs are no longer just datacenter creatures. We want them on laptops, phones, and NPUs—places where **latency**, **memory**, and **power** are as important as raw capability.

LFM2 is Liquid AI’s answer to that world: a family of “Liquid Foundation Models” designed with **hardware-in-the-loop** architecture search. The result is a **minimal hybrid**:

- Most layers: **gated short-range convolutions** (fast, local, cheap, no KV cache).
- A few layers: **grouped-query attention (GQA)** (global reasoning, efficient KV cache).
- Everywhere: **RMSNorm + SwiGLU MLPs**, plus MoE in larger models.

This post walks through the **math** of that architecture, in the same spirit as the linear-attention explainer that starts from $O = \mathrm{softmax}(QK^\top)V$ and peels everything back to clear equations. 

We’ll cover:

1. Notation & backbone layout  
2. Gated short convolution block  
3. Grouped-query attention (GQA)  
4. Full LFM2 layer equations  
5. MoE in LFM2-8B-A1B  
6. The vision–language connector (LFM2-VL)  
7. Audio stack (LFM2-Audio-1.5B)

![lfm2](/img/lfm2.png)

---

## 1. Notation and Backbone Layout

We consider a **decoder-only** stack operating on token sequences.

- Sequence length: $L$  
- Hidden size: $d$  
- Input: $h^{(0)} \in \mathbb{R}^{L \times d}$

Each LFM2 layer is a **pre-norm residual block** with two sub-blocks: a sequence block (conv or attention) and a position-wise MLP (SwiGLU). At layer $\ell$ we have:

$$
\begin{aligned}
u^{(\ell)} &= \mathrm{Norm}_1^{(\ell)}\!\big(h^{(\ell)}\big) \\
h^{(\ell+\tfrac{1}{2})} &= h^{(\ell)} + \mathcal{S}^{(\ell)}\!\big(u^{(\ell)}\big) \\
v^{(\ell)} &= \mathrm{Norm}_2^{(\ell)}\!\big(h^{(\ell+\tfrac{1}{2})}\big) \\
h^{(\ell+1)} &= h^{(\ell+\tfrac{1}{2})} + \mathcal{F}^{(\ell)}\!\big(v^{(\ell)}\big)
\end{aligned}
$$

- $\mathcal{S}^{(\ell)}$ is either a **gated short convolution block** or a **GQA block**.  
- $\mathcal{F}^{(\ell)}$ is a **SwiGLU feed-forward**.  
- Norms are **RMSNorm**; the model uses **RoPE + QK-Norm** in attention layers. 

The language modeling head is:

$$
\hat{y} = W_{\text{lm}} \cdot \mathrm{Norm}_{\text{final}}(h^{(L)}),
\qquad W_{\text{lm}} \in \mathbb{R}^{V \times d}
$$

with vocabulary size $V = 65{,}536$.

---

## 2. Gated Short Convolution Block

This is the main **local sequence operator** in LFM2. Given $h \in \mathbb{R}^{L \times d}$, the block is:

$$
(B, C, \tilde{h}) = \mathrm{Linear}(h),\quad 
y = B \odot \tilde{h},\quad
z = \mathrm{Conv}_k(y),\quad
o = \mathrm{Linear}_{\text{out}}(C \odot z)
$$

where:

- $\mathrm{Linear}: \mathbb{R}^d \to \mathbb{R}^{3d}$ is a **position-wise** projection split into three $d$-dim tensors $B, C, \tilde{h}$.  
- $\odot$ is element-wise multiplication (gating).  
- $\mathrm{Conv}_k$ is a **depthwise 1D convolution** along the sequence with kernel size $k$.  
- $\mathrm{Linear}_{\text{out}}$ is a position-wise output projection back to $\mathbb{R}^d$. 

### 2.1 Elementwise view

Let $h_t \in \mathbb{R}^d$ denote the hidden state at position $t$:

$$
[B_t, C_t, \tilde{h}_t] = W h_t + b
$$

with $B_t, C_t, \tilde{h}_t \in \mathbb{R}^d$. Then:

$$
\begin{aligned}
y_t &= B_t \odot \tilde{h}_t \\
(z_t)_j &= \sum_{\Delta=-r}^{r} K_{\Delta,j} \cdot (y_{t+\Delta})_j \\
o_t &= W_{\text{out}}(C_t \odot z_t) + b_{\text{out}}
\end{aligned}
$$

where:

- $k = 2r + 1$ is the convolution kernel size.  
- $K_{\Delta,j}$ is the depthwise conv weight for channel $j$ at offset $\Delta$.  

Each feature dimension $j$ has its own FIR filter across the window $[t-r, \dots, t+r]$.

### 2.2 Complexity and receptive field

- Time complexity: $\mathcal{O}(L \cdot d \cdot k)$  
- Memory complexity: $\mathcal{O}(L \cdot d)$ (no KV cache)  
- Receptive field: local; each position sees a window of size $k$.

In the hardware search, this operator consistently dominated more complex long-range operators (SSMs, linear attention, etc.) for **small on-device budgets**, once a few attention layers were allowed elsewhere for global reasoning. 

---

## 3. Grouped-Query Attention (GQA)

Convolutions are great for **local mixing**, but we still need **global** context for retrieval and reasoning. LFM2 uses **GQA** in a small number of layers to provide that.

### 3.1 Reminder: standard multi-head attention

For $h \in \mathbb{R}^{L \times d}$:

$$
Q = h W_Q,\quad K = h W_K,\quad V = h W_V
$$

Reshape into heads:

- $Q, K, V \in \mathbb{R}^{L \times H \times d_h}$ with $d_h = d / H$

Per head $i$:

$$
\mathrm{Attn}_i(h) = \mathrm{softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d_h}}\right) V_i
$$

### 3.2 GQA: shared KV per group

In **grouped-query attention**, we have:

- $H$ query heads  
- $G$ key/value groups with $G < H$  
- A mapping $g: \{1,\dots,H\} \to \{1,\dots,G\}$

We compute:

- $Q \in \mathbb{R}^{L \times H \times d_h}$ (as usual)  
- $K^{\text{group}}, V^{\text{group}} \in \mathbb{R}^{L \times G \times d_h}$

Head $i$ uses **its own queries** but **shared keys/values** from group $g(i)$:

$$
\mathrm{Attn}_i(h) = \mathrm{softmax}\!\left(\frac{Q_i K_{g(i)}^\top}{\sqrt{d_h}}\right) V_{g(i)}
$$

LFM2 also applies **QK-Norm** inside attention: each row of $Q$ and $K$ is normalized (e.g., to unit norm) before the dot product, which stabilizes the logits in deep stacks. 

### 3.3 KV cache savings

- Full MHA KV cache: $\mathcal{O}(L \cdot H \cdot d_h)$  
- GQA KV cache: $\mathcal{O}(L \cdot G \cdot d_h)$

Since $G \ll H$, this yields a significant **memory and bandwidth reduction**, which is crucial for on-device latency.

---

## 4. Full LFM2 Layer Equations

Let’s combine everything.

### 4.1 Sequence block

For a **conv layer**, the sequence block is the gated short convolution:

$$
\begin{aligned}
u^{(\ell)} &= \mathrm{RMSNorm}_1^{(\ell)}\big(h^{(\ell)}\big) \\
s^{(\ell)} &= \mathrm{GSC}^{(\ell)}\big(u^{(\ell)}\big) \\
h^{(\ell+\tfrac{1}{2})} &= h^{(\ell)} + s^{(\ell)}
\end{aligned}
$$

For an **attention layer**, replace $\mathrm{GSC}^{(\ell)}$ with the GQA operator:

$$
s^{(\ell)} = \mathrm{GQA}^{(\ell)}\big(u^{(\ell)}\big)
$$

### 4.2 MLP block (SwiGLU)

Given $v^{(\ell)} = \mathrm{RMSNorm}_2^{(\ell)}\big(h^{(\ell+\tfrac{1}{2})}\big)$, the position-wise **SwiGLU MLP** is:

$$
\begin{aligned}
[a, b] &= v^{(\ell)} W_1 + b_1 \\
\mathrm{SwiGLU}(v^{(\ell)}) &= \big(a \odot \mathrm{SiLU}(b)\big) W_2 + b_2 \\
h^{(\ell+1)} &= h^{(\ell+\tfrac{1}{2})} + \mathrm{SwiGLU}(v^{(\ell)})
\end{aligned}
$$

The architecture search tunes:

- Number and placement of **GSC vs GQA** layers  
- **Kernel sizes** $k$ in conv layers  
- **Expansion ratios** in the SwiGLU MLP  

subject to **Pareto-optimal trade-offs** between quality, latency, and memory on real devices.

---

## 5. MoE in LFM2-8B-A1B

LFM2-8B-A1B is a **Mixture-of-Experts** variant with:

- $8.3$B total parameters  
- $1.5$B active parameters per token 

Most layers use **sparse MoE feed-forward networks**. Each expert is a SwiGLU MLP, and the router chooses a small subset of experts per token.

### 5.1 Routing

For input $v_t \in \mathbb{R}^d$ to the FF block at position $t$:

$$
r_t = \mathrm{softmax}(W_r v_t),\quad r_t \in \mathbb{R}^E
$$

- $E$ = number of experts in that layer.  
- Take **Top-$K$** entries of $r_t$ to form the active expert set $T_t$.

Renormalize over $T_t$:

$$
\tilde{r}_{t,e} = 
\begin{cases}
\dfrac{r_{t,e}}{\sum\limits_{e' \in T_t} r_{t,e'}} & e \in T_t \\
0 & \text{otherwise}
\end{cases}
$$

### 5.2 Expert computation

Each expert $E_e(\cdot)$ is a SwiGLU block:

$$
E_e(v_t) = W_{2,e}\big((v_t W_{1,e}^{(a)}) \odot \mathrm{SiLU}(v_t W_{1,e}^{(b)})\big)
$$

The MoE output is:

$$
\mathcal{F}_{\text{MoE}}(v_t) = \sum_{e \in T_t} \tilde{r}_{t,e}\, E_e(v_t)
$$

In MoE layers, we simply replace the dense $\mathcal{F}^{(\ell)}$ feed-forward with $\mathcal{F}_{\text{MoE}}$.

This gives “$3$–$4$B-class” quality at roughly $1.5$B compute per token, staying within on-device constraints.   

---

## 6. Vision–Language: LFM2-VL

LFM2-VL adds vision via:

1. A **SigLIP2 NaFlex** image encoder  
2. A **PixelUnshuffle + MLP connector** into the language backbone  

### 6.1 Vision encoder

Given image $I$, the SigLIP2 encoder outputs patch embeddings:

$$
E = \phi_{\text{img}}(I) \in \mathbb{R}^{H_p \times W_p \times d_{\text{img}}}
$$

Flatten:

$$
E^{\text{flat}} \in \mathbb{R}^{(H_p W_p) \times d_{\text{img}}}
$$

### 6.2 PixelUnshuffle

PixelUnshuffle with factor $s$ reshapes:

$$
\mathbb{R}^{H_p \times W_p \times d_{\text{img}}}
\quad\longrightarrow\quad
\mathbb{R}^{(H_p/s) \times (W_p/s) \times (s^2 d_{\text{img}})}
$$

Effect:

- Number of tokens: divided by $s^2$  
- Channel dimension: multiplied by $s^2$

After PixelUnshuffle:

$$
E' = \mathrm{PixelUnshuffle}_s(E),\quad
E'^{\text{flat}} \in \mathbb{R}^{N_{\text{vis}} \times (s^2 d_{\text{img}})}
$$

LFM2-VL uses $s=2$ (4× fewer vision tokens) before the connector.   

### 6.3 Connector MLP

The connector maps to the language hidden size $d$:

$$
\begin{aligned}
z_i &= \sigma(E'^{\text{flat}}_i W_c^{(1)} + b_c^{(1)}) \\
v_i &= W_c^{(2)} z_i + b_c^{(2)} \in \mathbb{R}^d
\end{aligned}
$$

The resulting sequence $v \in \mathbb{R}^{N_{\text{vis}} \times d}$ becomes **visual tokens**. These are concatenated with text tokens (plus special tokens) and fed into the same LFM2 backbone, with RoPE providing positional structure.

---

## 7. Audio: LFM2-Audio-1.5B

LFM2-Audio reuses the LFM2-1.2B backbone and adds:

- An **audio encoder** (continuous audio $\to$ embeddings)  
- An **RVQ-based decoder** (discrete codes $\to$ waveform) with a **GAN detokenizer**   

### 7.1 Encoder: waveform to embeddings

Raw audio $x \in \mathbb{R}^T$ at 16 kHz:

1. Log-mel features:

$$
m = \mathrm{LogMel}(x) \in \mathbb{R}^{T_m \times 128}
$$

   with 25 ms windows and 10 ms stride.

2. Strided conv downsampling (8×):

$$
m' = \mathrm{ConvDownsample}(m) \in \mathbb{R}^{T_m' \times d_{\text{conv}}}
$$

3. 17 FastConformer layers:

$$
e = \mathrm{FastConf}_{17}(m') \in \mathbb{R}^{T_m' \times 512}
$$

4. MLP connector to LFM2 hidden size $d = 2048$:

$$
u_t = \phi_{\text{audio}}(e_t) 
= W_a^{(2)} \sigma(W_a^{(1)} e_t + b_a^{(1)}) + b_a^{(2)}
$$

Each $u_t$ represents about $0.08$ seconds of audio (12.5 embeddings per second) and is fed as a **token embedding** into the LFM2 backbone (optionally interleaved with text tokens). 

### 7.2 Decoder: embeddings to RVQ codes

On the output side, the model predicts **8 codebooks** of Mimi RVQ codes:

- 1 semantic codebook  
- 7 acoustic codebooks  
- Each codebook has $2048$ codes plus an EOS token (size $2049$).  

A single frame:

$$
c_t = (c_t^{(1)}, \dots, c_t^{(8)}),\quad c_t^{(i)} \in \{0, \dots, 2048\}
$$

encodes $0.08$ seconds of audio.

Instead of having the LFM2 backbone autoregress over all 8 codes directly, LFM2-Audio uses an **RQ-Transformer** conditioned on the backbone output:

1. LFM2 backbone produces a continuous embedding $h_t \in \mathbb{R}^{2048}$ at each time step.  
2. $h_t$ conditions a small RQ-Transformer that autoregressively generates the 8 codes in that frame:

$$
c_t = \mathrm{RQTransformer}(h_t)
$$

3. The 8 code tokens are embedded and summed back into a single vector, which is fed into the LFM2 backbone at the next step (closing the autoregressive loop).

### 7.3 Detokenizer: RVQ codes to waveform

Finally, the sequence $\{c_t\}_{t=1}^{T_f}$ is passed to a **Fourier-based GAN generator**:

$$
\hat{x} = \mathrm{GAN}_{\text{Fourier}}\big(\{c_t\}_{t=1}^{T_f}\big)
$$

to reconstruct the waveform.

So the full audio path is:

$$
\text{Waveform}
\;\xrightarrow{\text{encoder}}\;
\text{continuous embeddings}
\;\xrightarrow{\text{LFM2}}\;
\text{RVQ codes}
\;\xrightarrow{\text{GAN}}\;
\text{Waveform}
$$

---

## Closing Thoughts

If linear-attention took attention from

$$
O = \mathrm{softmax}(QK^\top)V
$$

down to compact state-update rules, LFM2 does something analogous **at the architecture level**:

- Replace most of the attention stack with **gated short convolutions**.  
- Keep only a handful of **GQA layers** for global reasoning.  
- Use **MoE**, **vision connectors**, and **audio encoders/decoders** that respect real hardware budgets.



